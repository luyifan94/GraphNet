{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, A, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.A_hat = A+torch.eye(A.size(0))\n",
    "        self.D = torch.diag(torch.sum(A,1))\n",
    "        self.D = self.D.inverse().sqrt()\n",
    "        self.W = nn.Parameter(torch.rand(in_feats, out_feats, requires_grad=True))\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = torch.mm(torch.mm(self.D, self.A_hat), self.D)\n",
    "        out = torch.mm(out, X)\n",
    "        out = torch.mm(out, self.W)\n",
    "        return out\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, adj, in_feats, hide_feats, out_feats):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gcn1 = GCNLayer(adj, in_feats, hide_feats)\n",
    "        self.gcn2 = GCNLayer(adj, hide_feats, out_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gcn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def load_cora():\n",
    "    raw_content = pd.read_csv('./data/cora/cora.content', sep='\\t', header=None)\n",
    "    num_nodes = raw_content.shape[0]\n",
    "\n",
    "    id = list(raw_content[0])\n",
    "    idx = list(raw_content.index)\n",
    "    id2idx = dict(zip(id, idx))\n",
    "\n",
    "    feat_data = raw_content.iloc[:, 1:-1]\n",
    "    feat_data = torch.FloatTensor(feat_data.values)\n",
    "    labels = pd.get_dummies(raw_content[1434])\n",
    "    labels = np.where(labels)[1]\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    raw_cites = pd.read_csv('./data/cora/cora.cites', sep='\\t', header=None)\n",
    "\n",
    "    adj = np.zeros((num_nodes, num_nodes))\n",
    "    for i, j in zip(raw_cites[0], raw_cites[1]):\n",
    "        x = id2idx[i]\n",
    "        y = id2idx[j]\n",
    "        adj[x][y] = adj[y][x] = 1\n",
    "    adj = torch.FloatTensor(adj)\n",
    "\n",
    "    idx_train = range(2000)\n",
    "    idx_val = range(2000, 2708)\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    \n",
    "    return feat_data, labels, adj, id2idx, idx_train, idx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_data, labels, adj, id2idx, idx_train, idx_val = load_cora()\n",
    "model = GCN(adj=adj, in_feats=feat_data.shape[1], hide_feats=16, out_feats=7)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 8.9460 acc_train: 0.2109 loss_val: 10.0348 acc_val: 0.2203 time: 0.3934s\n",
      "Epoch: 0002 loss_train: 8.7698 acc_train: 0.1736 loss_val: 9.7931 acc_val: 0.1723 time: 0.4062s\n",
      "Epoch: 0003 loss_train: 8.0958 acc_train: 0.1577 loss_val: 9.0525 acc_val: 0.1610 time: 0.3586s\n",
      "Epoch: 0004 loss_train: 6.7422 acc_train: 0.1780 loss_val: 7.6366 acc_val: 0.1794 time: 0.4043s\n",
      "Epoch: 0005 loss_train: 5.1445 acc_train: 0.2707 loss_val: 5.9815 acc_val: 0.2712 time: 0.3849s\n",
      "Epoch: 0006 loss_train: 4.0486 acc_train: 0.4645 loss_val: 4.8192 acc_val: 0.4068 time: 0.3973s\n",
      "Epoch: 0007 loss_train: 3.9014 acc_train: 0.3907 loss_val: 4.5827 acc_val: 0.3729 time: 0.3993s\n",
      "Epoch: 0008 loss_train: 3.9209 acc_train: 0.3944 loss_val: 4.5837 acc_val: 0.3686 time: 0.3818s\n",
      "Epoch: 0009 loss_train: 3.8632 acc_train: 0.3623 loss_val: 4.5110 acc_val: 0.3489 time: 0.3958s\n",
      "Epoch: 0010 loss_train: 3.4777 acc_train: 0.3996 loss_val: 4.0771 acc_val: 0.3743 time: 0.4053s\n",
      "Epoch: 0011 loss_train: 2.9543 acc_train: 0.4716 loss_val: 3.4912 acc_val: 0.4350 time: 0.3939s\n",
      "Epoch: 0012 loss_train: 2.4924 acc_train: 0.4871 loss_val: 2.9619 acc_val: 0.4562 time: 0.3973s\n",
      "Epoch: 0013 loss_train: 2.1001 acc_train: 0.4701 loss_val: 2.5054 acc_val: 0.4492 time: 0.3721s\n",
      "Epoch: 0014 loss_train: 1.7699 acc_train: 0.5166 loss_val: 2.1158 acc_val: 0.4887 time: 0.3818s\n",
      "Epoch: 0015 loss_train: 1.6822 acc_train: 0.5761 loss_val: 1.9827 acc_val: 0.5424 time: 0.3899s\n",
      "Epoch: 0016 loss_train: 2.0270 acc_train: 0.4956 loss_val: 2.2952 acc_val: 0.4633 time: 0.3720s\n",
      "Epoch: 0017 loss_train: 2.3606 acc_train: 0.4199 loss_val: 2.6364 acc_val: 0.3912 time: 0.3765s\n",
      "Epoch: 0018 loss_train: 2.2667 acc_train: 0.4261 loss_val: 2.5533 acc_val: 0.4040 time: 0.3988s\n",
      "Epoch: 0019 loss_train: 1.8481 acc_train: 0.5133 loss_val: 2.1377 acc_val: 0.4718 time: 0.3710s\n",
      "Epoch: 0020 loss_train: 1.3840 acc_train: 0.6108 loss_val: 1.6903 acc_val: 0.5749 time: 0.3914s\n",
      "Epoch: 0021 loss_train: 1.0767 acc_train: 0.6835 loss_val: 1.4087 acc_val: 0.6243 time: 0.3973s\n",
      "Epoch: 0022 loss_train: 0.9383 acc_train: 0.7053 loss_val: 1.2942 acc_val: 0.6384 time: 0.3948s\n",
      "Epoch: 0023 loss_train: 0.8855 acc_train: 0.7053 loss_val: 1.2636 acc_val: 0.6285 time: 0.3859s\n",
      "Epoch: 0024 loss_train: 0.8564 acc_train: 0.7020 loss_val: 1.2541 acc_val: 0.6045 time: 0.3641s\n",
      "Epoch: 0025 loss_train: 0.8302 acc_train: 0.7038 loss_val: 1.2426 acc_val: 0.6045 time: 0.3968s\n",
      "Epoch: 0026 loss_train: 0.8094 acc_train: 0.7116 loss_val: 1.2323 acc_val: 0.6158 time: 0.3983s\n",
      "Epoch: 0027 loss_train: 0.8053 acc_train: 0.7223 loss_val: 1.2351 acc_val: 0.6328 time: 0.3948s\n",
      "Epoch: 0028 loss_train: 0.8201 acc_train: 0.7242 loss_val: 1.2563 acc_val: 0.6370 time: 0.3929s\n",
      "Epoch: 0029 loss_train: 0.8429 acc_train: 0.7072 loss_val: 1.2837 acc_val: 0.6158 time: 0.4008s\n",
      "Epoch: 0030 loss_train: 0.8533 acc_train: 0.6942 loss_val: 1.2955 acc_val: 0.6031 time: 0.4043s\n",
      "Epoch: 0031 loss_train: 0.8346 acc_train: 0.6939 loss_val: 1.2735 acc_val: 0.6059 time: 0.4301s\n",
      "Epoch: 0032 loss_train: 0.7818 acc_train: 0.7057 loss_val: 1.2120 acc_val: 0.6186 time: 0.3983s\n",
      "Epoch: 0033 loss_train: 0.7042 acc_train: 0.7330 loss_val: 1.1195 acc_val: 0.6455 time: 0.4018s\n",
      "Epoch: 0034 loss_train: 0.6203 acc_train: 0.7770 loss_val: 1.0159 acc_val: 0.6893 time: 0.3809s\n",
      "Epoch: 0035 loss_train: 0.5482 acc_train: 0.8135 loss_val: 0.9227 acc_val: 0.7246 time: 0.3834s\n",
      "Epoch: 0036 loss_train: 0.4959 acc_train: 0.8431 loss_val: 0.8537 acc_val: 0.7641 time: 0.3963s\n",
      "Epoch: 0037 loss_train: 0.4640 acc_train: 0.8578 loss_val: 0.8117 acc_val: 0.7782 time: 0.3904s\n",
      "Epoch: 0038 loss_train: 0.4493 acc_train: 0.8582 loss_val: 0.7925 acc_val: 0.7768 time: 0.4102s\n",
      "Epoch: 0039 loss_train: 0.4470 acc_train: 0.8564 loss_val: 0.7902 acc_val: 0.7754 time: 0.4058s\n",
      "Epoch: 0040 loss_train: 0.4524 acc_train: 0.8567 loss_val: 0.7987 acc_val: 0.7768 time: 0.3884s\n",
      "Epoch: 0041 loss_train: 0.4612 acc_train: 0.8527 loss_val: 0.8123 acc_val: 0.7740 time: 0.3968s\n",
      "Epoch: 0042 loss_train: 0.4696 acc_train: 0.8508 loss_val: 0.8258 acc_val: 0.7768 time: 0.3983s\n",
      "Epoch: 0043 loss_train: 0.4742 acc_train: 0.8475 loss_val: 0.8350 acc_val: 0.7655 time: 0.3839s\n",
      "Epoch: 0044 loss_train: 0.4728 acc_train: 0.8479 loss_val: 0.8369 acc_val: 0.7684 time: 0.3844s\n",
      "Epoch: 0045 loss_train: 0.4647 acc_train: 0.8508 loss_val: 0.8304 acc_val: 0.7740 time: 0.3919s\n",
      "Epoch: 0046 loss_train: 0.4510 acc_train: 0.8582 loss_val: 0.8163 acc_val: 0.7825 time: 0.3884s\n",
      "Epoch: 0047 loss_train: 0.4339 acc_train: 0.8671 loss_val: 0.7972 acc_val: 0.7895 time: 0.3730s\n",
      "Epoch: 0048 loss_train: 0.4159 acc_train: 0.8748 loss_val: 0.7761 acc_val: 0.7994 time: 0.4023s\n",
      "Epoch: 0049 loss_train: 0.3992 acc_train: 0.8789 loss_val: 0.7559 acc_val: 0.8023 time: 0.3988s\n",
      "Epoch: 0050 loss_train: 0.3853 acc_train: 0.8781 loss_val: 0.7386 acc_val: 0.8051 time: 0.3700s\n",
      "Epoch: 0051 loss_train: 0.3749 acc_train: 0.8822 loss_val: 0.7256 acc_val: 0.8023 time: 0.3770s\n",
      "Epoch: 0052 loss_train: 0.3679 acc_train: 0.8863 loss_val: 0.7168 acc_val: 0.8023 time: 0.3819s\n",
      "Epoch: 0053 loss_train: 0.3637 acc_train: 0.8844 loss_val: 0.7119 acc_val: 0.8065 time: 0.3869s\n",
      "Epoch: 0054 loss_train: 0.3615 acc_train: 0.8833 loss_val: 0.7098 acc_val: 0.8037 time: 0.3775s\n",
      "Epoch: 0055 loss_train: 0.3603 acc_train: 0.8833 loss_val: 0.7094 acc_val: 0.8008 time: 0.3745s\n",
      "Epoch: 0056 loss_train: 0.3593 acc_train: 0.8837 loss_val: 0.7096 acc_val: 0.8051 time: 0.3874s\n",
      "Epoch: 0057 loss_train: 0.3579 acc_train: 0.8844 loss_val: 0.7095 acc_val: 0.8051 time: 0.3963s\n",
      "Epoch: 0058 loss_train: 0.3557 acc_train: 0.8848 loss_val: 0.7085 acc_val: 0.8051 time: 0.3953s\n",
      "Epoch: 0059 loss_train: 0.3527 acc_train: 0.8855 loss_val: 0.7066 acc_val: 0.8037 time: 0.3904s\n",
      "Epoch: 0060 loss_train: 0.3490 acc_train: 0.8863 loss_val: 0.7038 acc_val: 0.8037 time: 0.3914s\n",
      "Epoch: 0061 loss_train: 0.3450 acc_train: 0.8874 loss_val: 0.7005 acc_val: 0.8037 time: 0.3780s\n",
      "Epoch: 0062 loss_train: 0.3409 acc_train: 0.8885 loss_val: 0.6968 acc_val: 0.8051 time: 0.3849s\n",
      "Epoch: 0063 loss_train: 0.3368 acc_train: 0.8881 loss_val: 0.6931 acc_val: 0.8037 time: 0.3819s\n",
      "Epoch: 0064 loss_train: 0.3330 acc_train: 0.8888 loss_val: 0.6894 acc_val: 0.8065 time: 0.3864s\n",
      "Epoch: 0065 loss_train: 0.3294 acc_train: 0.8877 loss_val: 0.6860 acc_val: 0.8093 time: 0.3899s\n",
      "Epoch: 0066 loss_train: 0.3261 acc_train: 0.8892 loss_val: 0.6828 acc_val: 0.8150 time: 0.3884s\n",
      "Epoch: 0067 loss_train: 0.3230 acc_train: 0.8903 loss_val: 0.6797 acc_val: 0.8150 time: 0.3944s\n",
      "Epoch: 0068 loss_train: 0.3201 acc_train: 0.8900 loss_val: 0.6769 acc_val: 0.8150 time: 0.3701s\n",
      "Epoch: 0069 loss_train: 0.3172 acc_train: 0.8936 loss_val: 0.6742 acc_val: 0.8178 time: 0.3953s\n",
      "Epoch: 0070 loss_train: 0.3145 acc_train: 0.8966 loss_val: 0.6717 acc_val: 0.8220 time: 0.4103s\n",
      "Epoch: 0071 loss_train: 0.3119 acc_train: 0.8973 loss_val: 0.6695 acc_val: 0.8249 time: 0.3884s\n",
      "Epoch: 0072 loss_train: 0.3095 acc_train: 0.8984 loss_val: 0.6675 acc_val: 0.8249 time: 0.3581s\n",
      "Epoch: 0073 loss_train: 0.3072 acc_train: 0.8992 loss_val: 0.6658 acc_val: 0.8249 time: 0.3650s\n",
      "Epoch: 0074 loss_train: 0.3051 acc_train: 0.8999 loss_val: 0.6643 acc_val: 0.8249 time: 0.3745s\n",
      "Epoch: 0075 loss_train: 0.3032 acc_train: 0.8999 loss_val: 0.6630 acc_val: 0.8234 time: 0.3973s\n",
      "Epoch: 0076 loss_train: 0.3015 acc_train: 0.9010 loss_val: 0.6617 acc_val: 0.8263 time: 0.3948s\n",
      "Epoch: 0077 loss_train: 0.2999 acc_train: 0.9003 loss_val: 0.6605 acc_val: 0.8234 time: 0.3814s\n",
      "Epoch: 0078 loss_train: 0.2983 acc_train: 0.8999 loss_val: 0.6593 acc_val: 0.8206 time: 0.3979s\n",
      "Epoch: 0079 loss_train: 0.2968 acc_train: 0.8996 loss_val: 0.6579 acc_val: 0.8206 time: 0.3809s\n",
      "Epoch: 0080 loss_train: 0.2952 acc_train: 0.8999 loss_val: 0.6564 acc_val: 0.8206 time: 0.3954s\n",
      "Epoch: 0081 loss_train: 0.2936 acc_train: 0.9003 loss_val: 0.6548 acc_val: 0.8206 time: 0.4003s\n",
      "Epoch: 0082 loss_train: 0.2919 acc_train: 0.9010 loss_val: 0.6530 acc_val: 0.8192 time: 0.3760s\n",
      "Epoch: 0083 loss_train: 0.2902 acc_train: 0.9010 loss_val: 0.6511 acc_val: 0.8192 time: 0.4048s\n",
      "Epoch: 0084 loss_train: 0.2886 acc_train: 0.9014 loss_val: 0.6492 acc_val: 0.8192 time: 0.3715s\n",
      "Epoch: 0085 loss_train: 0.2869 acc_train: 0.9014 loss_val: 0.6473 acc_val: 0.8178 time: 0.3695s\n",
      "Epoch: 0086 loss_train: 0.2853 acc_train: 0.9036 loss_val: 0.6454 acc_val: 0.8206 time: 0.3685s\n",
      "Epoch: 0087 loss_train: 0.2837 acc_train: 0.9032 loss_val: 0.6437 acc_val: 0.8206 time: 0.3616s\n",
      "Epoch: 0088 loss_train: 0.2822 acc_train: 0.9025 loss_val: 0.6420 acc_val: 0.8178 time: 0.3834s\n",
      "Epoch: 0089 loss_train: 0.2809 acc_train: 0.9025 loss_val: 0.6405 acc_val: 0.8178 time: 0.3566s\n",
      "Epoch: 0090 loss_train: 0.2796 acc_train: 0.9032 loss_val: 0.6392 acc_val: 0.8178 time: 0.3795s\n",
      "Epoch: 0091 loss_train: 0.2783 acc_train: 0.9025 loss_val: 0.6379 acc_val: 0.8178 time: 0.3730s\n",
      "Epoch: 0092 loss_train: 0.2772 acc_train: 0.9029 loss_val: 0.6367 acc_val: 0.8192 time: 0.3854s\n",
      "Epoch: 0093 loss_train: 0.2760 acc_train: 0.9021 loss_val: 0.6356 acc_val: 0.8178 time: 0.3919s\n",
      "Epoch: 0094 loss_train: 0.2749 acc_train: 0.9021 loss_val: 0.6345 acc_val: 0.8178 time: 0.3800s\n",
      "Epoch: 0095 loss_train: 0.2738 acc_train: 0.9021 loss_val: 0.6335 acc_val: 0.8178 time: 0.4083s\n",
      "Epoch: 0096 loss_train: 0.2727 acc_train: 0.9029 loss_val: 0.6324 acc_val: 0.8178 time: 0.3953s\n",
      "Epoch: 0097 loss_train: 0.2715 acc_train: 0.9032 loss_val: 0.6313 acc_val: 0.8178 time: 0.3814s\n",
      "Epoch: 0098 loss_train: 0.2704 acc_train: 0.9029 loss_val: 0.6302 acc_val: 0.8164 time: 0.3730s\n",
      "Epoch: 0099 loss_train: 0.2693 acc_train: 0.9025 loss_val: 0.6291 acc_val: 0.8164 time: 0.3701s\n",
      "Epoch: 0100 loss_train: 0.2681 acc_train: 0.9036 loss_val: 0.6281 acc_val: 0.8192 time: 0.3993s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(feat_data)\n",
    "\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "\n",
    "    acc_train = accuracy(output, labels)\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "        'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "        'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "        'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "        'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "        'time: {:.4f}s'.format(time.time() - t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "694a5ca4ff500403e563beeba6426a1c919db37382c2fe6c52fb893497b50295"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
